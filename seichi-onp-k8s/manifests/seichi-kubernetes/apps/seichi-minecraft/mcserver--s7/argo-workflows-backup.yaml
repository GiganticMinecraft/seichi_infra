apiVersion: argoproj.io/v1alpha1
kind: CronWorkflow
metadata:
  name: backup--mcserver--s7
  namespace: seichi-minecraft
spec:
  schedule: "0 4 * * *"
  timezone: Asia/Tokyo
  concurrencyPolicy: "Forbid"
  workflowSpec:
    workflowTemplateRef:
      name: backup--mcserver--s7
---
apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: backup--mcserver--s7
  namespace: seichi-minecraft
  annotations:
    workflows.argoproj.io/description: "mcserver--s7のデータをProxmox Backup Serverにバックアップするワークフロー"
spec:
  serviceAccountName: mcserver--s7-workflow-sa
  ttlStrategy:
    secondsAfterCompletion: 86400
  entrypoint: default
  templates:
    - name: default
      steps:
        - - name: generate-timestamp
            template: get-timestamp
        - - name: patch-statefulset-to-0
            template: patch-statefulset-to-0
        - - name: wait-for-scale-to-0
            template: wait-for-scale-to-0
        - - name: run-backup
            template: run-backup
        - - name: delete-coreprotect--s7-db
            template: delete-coreprotect--s7-db
        - - name: patch-statefulset-to-1
            template: patch-statefulset-to-1
        - - name: wait-for-scale-to-1
            template: wait-for-scale-to-1

    - name: get-timestamp
      script:
        image: mirror.gcr.io/alpine
        command: [sh]
        source: |
          echo $(date +%Y%m%d-%H%M%S)

    - name: patch-statefulset-to-0
      script:
        image: mirror.gcr.io/bitnami/kubectl:1.33.1
        command: ["/bin/sh", "-c"]
        source: |
          kubectl patch statefulset mcserver--s7 -n seichi-minecraft --type='merge' -p '{"spec": {"replicas": 0}}'

    - name: wait-for-scale-to-0
      script:
        image: mirror.gcr.io/bitnami/kubectl:1.33.1
        command: ["/bin/sh", "-c"]
        source: |
          echo "Waiting for statefulset to scale down..."
          while true; do
            replicas=$(kubectl get statefulset mcserver--s7 -n seichi-minecraft -o jsonpath='{.status.availableReplicas}')
            if [ -z "$replicas" ] || [ "$replicas" -eq 0 ]; then
              break
            fi
            echo "Still scaling down..."
            sleep 5
          done
          echo "Scale-down confirmed!"

    - name: run-backup
      script:
        image: mirror.gcr.io/debian:12
        command: ["/bin/sh", "-c"]
        source: |
          set -e
          
          echo "Updating package lists..."
          apt update
          
          echo "Installing packages..."
          apt install -y curl
          
          echo "Adding Proxmox restore Client repository..."
          echo "deb http://download.proxmox.com/debian/pbs-client bookworm main" > /etc/apt/sources.list.d/pbs-client.list
          curl -fsSL https://enterprise.proxmox.com/debian/proxmox-release-bookworm.gpg -o /etc/apt/trusted.gpg.d/proxmox-release-bookworm.gpg
          
          echo "Updating package lists..."
          apt update
          
          echo "Installing proxmox-backup-client version 3.3.2-1..."
          apt install -y proxmox-backup-client=3.3.2-1
          
          echo "Running backup..."
          proxmox-backup-client backup "${BACKUP_NAME}" \
            --repository "${PBS_USER}@${PBS_HOST}:${PBS_DATASTORE}" \
            --backup-id "${BACKUP_ID}"
        env:
          - name: TZ
            value: "Asia/Tokyo"
          - name: BACKUP_NAME
            value: "data.pxar:/data"
          # proxmox-backup-server側でどのサーバのバックアップかを識別するためにサーバごとに異なるbackup-idを指定する
          - name: BACKUP_ID
            value: "mcserver--s7"
          - name: PBS_USER
            valueFrom:
              secretKeyRef:
                name: pbs-credentials
                key: user
          - name: PBS_HOST
            valueFrom:
              secretKeyRef:
                name: pbs-credentials
                key: host
          - name: PBS_DATASTORE
            valueFrom:
              secretKeyRef:
                name: pbs-credentials
                key: datastore
          - name: PBS_PASSWORD
            valueFrom:
              secretKeyRef:
                name: pbs-credentials
                key: password
          - name: PBS_FINGERPRINT
            valueFrom:
              secretKeyRef:
                name: pbs-credentials
                key: fingerprint
        volumeMounts:
          - name: backup-target-volume
            mountPath: /data
          # /root/.cache が overlayfs 配下だとスクリプトがこけるので O_TMPFILE フラグ付きの openat(2) が実行可能なファイルシステムをマウントする
          # refs: https://gist.github.com/unchama/0922fce3c490e46b6f9e822f4377853e
          - name: pbs-tmpfs
            mountPath: /root/.cache

    - name: delete-coreprotect--s7-db
      script:
        image: mirror.gcr.io/bitnami/kubectl:1.33.1
        command: ["/bin/sh", "-c"]
        source: |
          kubectl delete database coreprotect-s7 -n seichi-minecraft --wait=true

    - name: patch-statefulset-to-1
      script:
        image: mirror.gcr.io/bitnami/kubectl:1.33.1
        command: ["/bin/sh", "-c"]
        source: |
          kubectl patch statefulset mcserver--s7 -n seichi-minecraft --type='merge' -p '{"spec": {"replicas": 1}}'

    - name: wait-for-scale-to-1
      activeDeadlineSeconds: 300 # 5分待っても上がってこない場合は諦める
      script:
        image: mirror.gcr.io/bitnami/kubectl:1.33.1
        command: ["/bin/sh", "-c"]
        source: |
          echo "Waiting for StatefulSet to scale up..."
          while [ "$(kubectl get statefulset mcserver--s7 -n seichi-minecraft -o jsonpath='{.status.availableReplicas}')" != "1" ]; do
            echo "Still scaling up..."
            sleep 5
          done
          echo "Scale-up confirmed!"
  volumes:
    - name: backup-target-volume
      persistentVolumeClaim:
        claimName: mcserver--s7-data-mcserver--s7-0
    - name: pbs-tmpfs
      emptyDir: {}
