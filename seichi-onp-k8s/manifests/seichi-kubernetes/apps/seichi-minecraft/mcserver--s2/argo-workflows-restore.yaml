apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: restore--mcserver--s2
  namespace: seichi-minecraft
  annotations:
    workflows.argoproj.io/description: "mcserver--s2のデータをProxmox Backup Serverからリストアするワークフロー"
    workflows.argoproj.io/parameters: |
      RESTORE_TARGET_BACKUP_ID_PBS: "リストア元として使用するPBSバックアップID (デフォルトはargo-workflowでバックアップしたIDなので別のバックアップを使用したい時のみ変更する)"
      RESTORE_TARGET_DATE_PBS: "リストアするPBSバックアップの日付 (例: 2024-02-10T12:00:00Z)"
spec:
  serviceAccountName: mcserver--s2-workflow-sa
  ttlStrategy:
    secondsAfterCompletion: 86400
  entrypoint: default
  arguments:
    parameters:
      - name: RESTORE_TARGET_BACKUP_ID_PBS
        description: "リストア元として使用するPBSバックアップID (デフォルトはargo-workflowでバックアップしたIDなので別のバックアップを使用したい時のみ変更する)"
        value: "mcserver--s2"  # 初期値は argo-workflows-backup.yaml で指定したID
      - name: RESTORE_TARGET_DATE_PBS
        description: "リストアするPBSバックアップの日付 (例: 2024-02-10T12:00:00Z)"
        value: ""  # 初期値を空にしておく
  templates:
    - name: default
      steps:
        - - name: generate-timestamp
            template: get-timestamp
        - - name: patch-statefulset-to-0
            template: patch-statefulset-to-0
        - - name: wait-for-scale-to-0
            template: wait-for-scale-to-0
        - - name: run-restore
            template: run-restore
        - - name: patch-statefulset-to-1
            template: patch-statefulset-to-1
        - - name: wait-for-scale-to-1
            template: wait-for-scale-to-1

    - name: get-timestamp
      script:
        image: alpine
        command: [sh]
        source: |
          echo $(date +%Y%m%d-%H%M%S)
  
    - name: patch-statefulset-to-0
      script:
        image: bitnami/kubectl:1.32.3
        command: ["/bin/sh", "-c"]
        source: |
          kubectl patch statefulset mcserver--s2 -n seichi-minecraft --type='merge' -p '{"spec": {"replicas": 0}}'

    - name: wait-for-scale-to-0
      script:
        image: bitnami/kubectl:1.32.3
        command: ["/bin/sh", "-c"]
        source: |
          echo "Waiting for statefulset to scale down..."
          while true; do
            replicas=$(kubectl get statefulset mcserver--s2 -n seichi-minecraft -o jsonpath='{.status.availableReplicas}')
            if [ -z "$replicas" ] || [ "$replicas" -eq 0 ]; then
              break
            fi
            echo "Still scaling down..."
            sleep 5
          done
          echo "Scale-down confirmed!"

    - name: run-restore
      script:
        image: debian:12
        command: ["/bin/sh", "-c"]
        source: |
          set -e
          
          echo "Updating package lists..."
          apt update
          
          echo "Installing packages..."
          apt install -y curl
          
          echo "Adding Proxmox restore Client repository..."
          echo "deb http://download.proxmox.com/debian/pbs-client bookworm main" > /etc/apt/sources.list.d/pbs-client.list
          curl -fsSL https://enterprise.proxmox.com/debian/proxmox-release-bookworm.gpg -o /etc/apt/trusted.gpg.d/proxmox-release-bookworm.gpg
          
          echo "Updating package lists..."
          apt update
          
          echo "Installing proxmox-backup-client version 3.3.2-1..."
          apt install -y proxmox-backup-client=3.3.2-1
          
          echo "Deleting RESTORE_TARGET_DIR before restore..."
          rm -vrf ${RESTORE_TARGET_DIR}/*
          ls -a ${RESTORE_TARGET_DIR}/ | egrep -v '^.$' | egrep -v '^..$' | xargs -I {} rm -vrf ${RESTORE_TARGET_DIR}/{}

          echo "Running restore to RESTORE_TARGET_DIR..."
          proxmox-backup-client restore "host/${BACKUP_ID}/{{workflow.parameters.RESTORE_TARGET_DATE_PBS}}" \
            "${RESTORE_ARCHIVE_NAME}" "${RESTORE_TARGET_DIR}" \
            --repository "${PBS_USER}@${PBS_HOST}:${PBS_DATASTORE}"
        env:
          - name: TZ
            value: "Asia/Tokyo"
          - name: RESTORE_TARGET_DATE_PBS
            value: "{{workflow.parameters.RESTORE_TARGET_DATE_PBS}}"
          - name: RESTORE_ARCHIVE_NAME
            value: "data.pxar"
          - name: RESTORE_TARGET_DIR
            value: "/data"
          - name: BACKUP_ID
            value: "{{workflow.parameters.RESTORE_TARGET_BACKUP_ID_PBS}}"
          - name: PBS_USER
            valueFrom:
              secretKeyRef:
                name: pbs-credentials
                key: user
          - name: PBS_HOST
            valueFrom:
              secretKeyRef:
                name: pbs-credentials
                key: host
          - name: PBS_DATASTORE
            valueFrom:
              secretKeyRef:
                name: pbs-credentials
                key: datastore
          - name: PBS_PASSWORD
            valueFrom:
              secretKeyRef:
                name: pbs-credentials
                key: password
          - name: PBS_FINGERPRINT
            valueFrom:
              secretKeyRef:
                name: pbs-credentials
                key: fingerprint
        volumeMounts:
          - name: restore-target-volume
            mountPath: /data
          # /root/.cache が overlayfs 配下だとスクリプトがこけるので O_TMPFILE フラグ付きの openat(2) が実行可能なファイルシステムをマウントする
          # refs: https://gist.github.com/unchama/0922fce3c490e46b6f9e822f4377853e
          - name: pbs-tmpfs
            mountPath: /root/.cache
  
    - name: patch-statefulset-to-1
      script:
        image: bitnami/kubectl:1.32.3
        command: ["/bin/sh", "-c"]
        source: |
          kubectl patch statefulset mcserver--s2 -n seichi-minecraft --type='merge' -p '{"spec": {"replicas": 1}}'

    - name: wait-for-scale-to-1
      activeDeadlineSeconds: 300 # 5分待っても上がってこない場合は諦める
      script:
        image: bitnami/kubectl:1.32.3
        command: ["/bin/sh", "-c"]
        source: |
          echo "Waiting for StatefulSet to scale up..."
          while [ "$(kubectl get statefulset mcserver--s2 -n seichi-minecraft -o jsonpath='{.status.availableReplicas}')" != "1" ]; do
            echo "Still scaling up..."
            sleep 5
          done
          echo "Scale-up confirmed!"
  volumes:
    - name: restore-target-volume
      persistentVolumeClaim:
        claimName: mcserver--s2-data-mcserver--s2-0
    - name: pbs-tmpfs
      emptyDir: {}
